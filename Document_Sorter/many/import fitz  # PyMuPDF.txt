import fitz  # PyMuPDF
import os
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
import joblib

# Download stopwords (only once)
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def extract_text_from_pdf(pdf_path):
    """Extract text from a PDF file."""
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text.strip()

def clean_text(text):
    """Clean extracted text: remove numbers, special characters, and stopwords."""
    text = text.lower()
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return " ".join(words)

# ðŸ”¹ Load trained model & vectorizer
model = joblib.load("document_classifier.pkl")
vectorizer = joblib.load("vectorizer.pkl")

# ðŸ”¹ Folder containing PDFs
pdf_folder = "C:\\Users\\Vinod Makkala\\OneDrive\\Desktop\\many"

# ðŸ”¹ Process all PDFs in the folder
for filename in os.listdir(pdf_folder):
    if filename.endswith(".pdf"):  
        file_path = os.path.join(pdf_folder, filename)

        # Extract & clean text
        pdf_text = extract_text_from_pdf(file_path)
        pdf_text_cleaned = clean_text(pdf_text)

        # Transform text into features & classify
        X_input = vectorizer.transform([pdf_text_cleaned])
        prediction = model.predict(X_input)[0]
        
        # Label output
        label = "Important" if prediction == 1 else "Unimportant"
        
        print(f"ðŸ“Œ {filename} --> {label}")
